{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divar NLP Workshop - Winter 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting ads Category Based on Descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this analysis is to train different classifier to predict ads category based on their title and descriptions. The sections of this analysis include:\n",
    "- split train and test data\n",
    "- Use tfidf vectorization\n",
    "  * train a basic classifier with a MultinomialNavieBayes and become familiar with scikit pipelines.\n",
    "  * explore the results\n",
    "  * Use other classifications methods by tf-idf and compare the results\n",
    "  * Choose the best one and improve it\n",
    "- Use word embeddings technic to classify\n",
    "    - train a classifier with fasttext\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Thing First: Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext wurlitzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we we want to predict ```cat2``` based on ```preprocessed_text```. So read these columns from your processed data in part 1 of the workshop, (```\"outputs/preprocessed_data.parquet\"```) and then remove the records that their cat2 is null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "data = # read data\n",
    "data = # filter the records that their cat2 is null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check number of each label in dataset. what are most and least popular cat2? Did a particular point catch your attention? Yeah, It seem that our dataset is unbalanced and we should pay attention to this in future. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Train, validation and Test Datasets\n",
    "\n",
    "Use any desired method to create *train*, and *test* datasets. Use $60\\%$ of whole data at random as *train*, $20\\%$ in *validations* and others as *test* datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "train, validation, test = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may also want to save your train, validation and test datasets in `.parquet` format under the `./outputs/classification_task/` path for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir outputs/classification_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your train, validation and test data is ready now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now load saved datasets. and remove the records which their cat2 column is null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_parquet('./outputs/classification_task/train.parquet')\n",
    "validation = pd.read_parquet('./outputs/classification_task/validation.parquet')\n",
    "test = pd.read_parquet('./outputs/classification_task/test.parquet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill the below variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "X_train = \n",
    "X_validation = \n",
    "X_test = \n",
    "y_train = \n",
    "y_validation = \n",
    "y_test = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a model by scikit pipiline and CountVectorizer, tfidfTransformer MultinimialNB to predict ```cat2``` based on ```preprocessed_text``` columns. Maybe you should want to read more about sklearn pipeline here:\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fill ... parts\n",
    "\n",
    "text_clf = Pipeline([('vect', ...),\n",
    "                     ('tfidf', ...),\n",
    "                     ('clf', ...),\n",
    "                     ])\n",
    "\n",
    "text_clf.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use predict function of ```text_clf``` to predict ```cat2``` on validation dataset. and then use ```sklearn.metrics.classification_report``` to see some metrics of your classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "\n",
    "preds_validation = \n",
    "\n",
    "print(metrics.classification_report(list(y_validation), preds_validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print the size of vocabularies by ```len(pipeline.named_steps['count_vect'].vocabulary_.keys())```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's change the min_df and see the effect of vocabulary size in test performance. For example try with min_df=50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you interpret this difference? It seems that it is important for search on some important hyperparameters of the models. \n",
    "\n",
    "There are different methods and packages for hyperparameter tuning but here let try it by ```sklearn GridSearchCV``` class and become more familiar with sklearn pipelines.\n",
    "\n",
    "As you remember we had imbalanced datasets and maybe the MultinomialNB fit_prior parameters could be helpful to reduce its impact. First become sure that you know the meaning of fit_prior in NB and then use a GridSearchCV on min_df parameter in range [1, 10 , 50] and fit_prior in range [False, True], use n_splits=3 and scoring='f1_macro'. (what is the difference between f1_macro and f1_weighted?)\n",
    "\n",
    "Increasing n_jobs can be an option for faster training. But if you still spend a lot of time on training, you can also use a sample of train data in this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "# TODO:\n",
    "estimator = \n",
    "param_grid = \n",
    "scoring = \n",
    "n_jobs = \n",
    "n_splits = 3\n",
    "\n",
    "pipeline = GridSearchCV(\n",
    "    estimator=esitmator,\n",
    "    param_grid=param_grid,\n",
    "    cv=StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42),\n",
    "    scoring=scoring,\n",
    "    verbose=2, \n",
    "    n_jobs=n_jobs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outputs of GridSearcCV.fit has interesting information about your different models. Read and use its attributes from:\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What was the best_paramaters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more detailed information see pipeline.cv_results_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the best estimator and compute classification report on validation dataset and compare it with previous results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before deep dive into MultinomialNB try to fit some other models. For example try LinearSVC, LogisticRegression and MultinomialNB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifiers = [\n",
    "    ('naive_bayes', MultinomialNB),\n",
    "    ('lr', LogisticRegression),\n",
    "    ('svc', LinearSVC),\n",
    "    \n",
    "]\n",
    "\n",
    "classifier_dict = dict()\n",
    "\n",
    "\n",
    "for name, clf in classifiers:\n",
    "    print(name)\n",
    "    # TODO:\n",
    "    classifier_dict[name] = \n",
    "    ...\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save the best model to ```./outputs/best_model.pkl```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "# TODO:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = joblib.load('./outputs/best_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should spend more time on the hyperparameter tuning of these models. But we recommend you to continue this part yourself later. And now let's take a closer look at the best model we have and analyze the sources of it's error.\n",
    "\n",
    "In the first step, try to plot confusion matrix of the model's predictions using ```plot_confusion_matrix``` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    \n",
    "\n",
    "    if normalize:\n",
    "        cm = confusion_matrix(y_true, y_pred, normalize='true')\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "#     print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(16, 12))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=50, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_validation = best_model.predict(X_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot normalized confusion matrix\n",
    "# TODO:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try to identify areas where the model is not performing well and then see examples of misclassifications in that areas in your validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to write a functions to show most important words for each class. In sklearn pipeline you could access to each steps of pipeline by ```.named_steps['step_name']``` and then you have access to attributes of each step.\n",
    "\n",
    "In this example after accessing to CountVectorizer step you can access to word names by its function ```get_feature_names()```. And  ```LinearSVC.coef_``` is useful for finding most important coefficients for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = pipeline.best_estimator_.named_steps['clf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn import svm\n",
    "# TODO:\n",
    "\n",
    "def f_importances(estimator, k, class_name):\n",
    "    # it should return list of top k important words and their scores for class_name for example for class_name='animal'\n",
    "    names = estimator.named_steps['vect'].get_feature_names()\n",
    "    clf = estimator.named_steps['clf']\n",
    "    class_index = list(clf.classes_).index(class_name)\n",
    "    # TODO: create imp and names to return\n",
    "    \n",
    "    return imp, names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp, names = f_importances(best_model, 40, 'cars')\n",
    "print(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - The End."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "divar-nlp-kernel",
   "language": "python",
   "name": "divar-nlp-workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
