{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divar NLP Workshop - Winter 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Similar Ads Based on Title and Descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this analysis is to use different Natural Language Processing methods to find similar ads based on their descriptions. The sections of this analysis include:\n",
    "- Preprocessing the text\n",
    "- Transforming the text into vectors\n",
    "    - Method 1: TfidfVectorizer \n",
    "    - Method 2: word2vec & Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl\" >\n",
    "<!--     <div style=\"font-size:180%\"> -->\n",
    "<!--         <h3>بخش اول: پیش‌پردازش داده</h3> -->\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "# First Part: Preprocessing  Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Thing First: Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext wurlitzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have included a pandas dataframe into ```divar_ads_dataset.csv``` which is our main data today.\n",
    "\n",
    "Please load ```input-data/divar_advertisements_v1.0/divar_ads_dataset.csv``` into ```data```.You may want to use ```pd.read_csv``` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "data ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use ```head``` to see first 4 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For better displaying table information for you can use ```pd.set_option```. For example by using this command ```pd.set_option(\"display.max_colwidth\", 200)``` you can see entire text column informations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some packages like ```pandas_profiling``` that help you undersntand your data better in just a few lines of codes. Using these packages could be boost your exploratory data analysis step.\n",
    "Please see https://github.com/pandas-profiling/pandas-profiling and generate a report on html file for this dataset and save it on ```outputs/dataset_profile.html```. If it takes a lot of time to prepare the report pass only 10000 sample of data and see the results.\n",
    "Check out this report for a few minutes and make some interesting points that come to mind. For example:\n",
    "* what is the data type of each column?\n",
    "* which variables has missing values?\n",
    "* What is the average length of desc?\n",
    "* which varialbles have high correlations and why?\n",
    "* What do you find interesting in price report?\n",
    "* ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add a new column by concatinate the title and desc columns and name the new column the ```text```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the **text** of the first row from data into ```text``` and print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to preprocess the ```text``` and convert it to list of it's tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cook `text`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use ```Normalizer```, ```Lemmatizer```, and ```WordTokenizer``` classes from **Hazm**.\n",
    "\n",
    "\n",
    "See examples below. They are from `Hazm` Repo in github. \n",
    "Check this link [Hazm-GitHub](https://github.com/sobhe/hazm) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    ">>> from __future__ import unicode_literals\n",
    ">>> from hazm import *\n",
    "\n",
    ">>> normalizer = Normalizer()\n",
    ">>> normalizer.normalize('اصلاح نويسه ها و استفاده از نیم‌فاصله پردازش را آسان مي كند')\n",
    "'اصلاح نویسه‌ها و استفاده از نیم‌فاصله پردازش را آسان می‌کند'\n",
    "\n",
    ">>> word_tokenize('ولی برای پردازش، جدا بهتر نیست؟')\n",
    "['ولی', 'برای', 'پردازش', '،', 'جدا', 'بهتر', 'نیست', '؟']\n",
    "\n",
    ">>> lemmatizer = Lemmatizer()\n",
    ">>> lemmatizer.lemmatize('می‌روم')\n",
    "'رفت#رو'\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Normalization and Word Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from hazm import Normalizer, WordTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new object of ```Normalizer``` and try to identify the role of each parameters of Normalizer Class, you could use ```??Normalizer``` to see its documentation. Then do the same thing for ```wordTokenizer```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "??Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "??WordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "normalizer = \n",
    "wordTokenizer = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract list of words from ```text```.\n",
    "In order to do so, first you have to normalize it using ```normalizer```, and then tokenize it using `wordTokenizer`.\n",
    "\n",
    "Pass `text` to `normalizer.normalize`, and then pass the results to `wordTokenizer.tokenize`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO \n",
    "words = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print first 5 `words`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = '➊ + ➋ = ➂'\n",
    "print (normalizer.normalize(sample_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you guess about the problem ?\n",
    "\n",
    "\n",
    "If you find the ```normalizer.translation```, you could see that there is a dictionary from non-standard-char to standard-char that ```Normalizer``` use it for normalize characters to standard ones. \n",
    "\n",
    "We have an expanded version of this dictionary. You could find it on ```new_translation```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('input-data/my_translation_dict.pickle', 'rb') as handle:\n",
    "    new_translation= pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please reform above ```normalization and tokenization``` steps to use this version of translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hazm import Lemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to get each word from `words` lemmatized.\n",
    "\n",
    "First create a `Lemmatizer` object. Then pass **each word** in `words` to `lemmatizer.lemmatize` and create a new list, `lemmatized_words`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "lemmatizer = \n",
    "lemmatized_words = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run cell below to see first 20 words and their lemmatized form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for word in words[1:20]:\n",
    "    print(\"word= %s ; lemmatized= %s\" % (word, lemmatizer.lemmatize(word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing stop words from data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some useless words in corpus, we call them stop-words.\n",
    "\n",
    "Run cell below to load `input-data/stopwords.dat` into `stopwords`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "def stopwords_list(stopwords_file):\n",
    "    with codecs.open(stopwords_file, encoding='utf8') as stopwords_file:\n",
    "        return list(map(lambda w: w.strip(), stopwords_file))\n",
    "stopwords = set(stopwords_list(\"input-data/stopwords.dat\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a new list, `lemmatized_without_stopwords`. \n",
    "\n",
    "Put **each non-stop word** in `lemmatized_words`, into `lemmatized_without_stopwords`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "lemmatized_without_stopwords = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run cell below to see some stop-words from original text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(set(lemmatized_words)-set(lemmatized_without_stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to clean all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import hazm\n",
    "\n",
    "\n",
    "def compile_patterns(patterns):\n",
    "    return [(re.compile(pattern), repl) for (pattern, repl) in patterns]\n",
    "\n",
    "\n",
    "def maketrans(src_chars, dest_chars):\n",
    "    return dict((ord(a), b) for a, b in zip(src_chars, dest_chars))\n",
    "\n",
    "\n",
    "class TextHandler:\n",
    "    def __init__(self, persian_numbers=False,\n",
    "                 change_lang_spacing=True,\n",
    "                 remove_non_standard_char=True,\n",
    "                 remove_repetitive_chars=True,\n",
    "                 text_refinement_patterns=None,\n",
    "                 user_translations=None):\n",
    "        # text preprocessing config\n",
    "        if not persian_numbers:\n",
    "            number_src = '۰۱۲۳۴۵۶۷۸۹٪'\n",
    "            number_dest = '0123456789%'\n",
    "        else:\n",
    "            number_dest = '۰۱۲۳۴۵۶۷۸۹٪'\n",
    "            number_src = '0123456789%'\n",
    "        \n",
    "        self.number_translations = maketrans(number_src, number_dest)\n",
    "        \n",
    "        if not user_translations:\n",
    "            self.user_translations = dict()\n",
    "        else:\n",
    "            self.user_translations = user_translations\n",
    "\n",
    "        self._remove_repetitive_chars = remove_repetitive_chars\n",
    "        self._change_lang_spacing = change_lang_spacing\n",
    "        self._remove_non_standard_char = remove_non_standard_char\n",
    "        self.text_normalizer = hazm.Normalizer(remove_extra_spaces=True,\n",
    "                                               persian_style=False,\n",
    "                                               persian_numbers=False,\n",
    "                                               remove_diacritics=True,\n",
    "                                               affix_spacing=True,\n",
    "                                               token_based=False,\n",
    "                                               punctuation_spacing=True)\n",
    "\n",
    "        self.word_tokenizer = hazm.WordTokenizer(join_verb_parts=False, separate_emoji=True,\n",
    "                                                 replace_links=True,\n",
    "                                                 replace_IDs=False,\n",
    "                                                 replace_emails=True,\n",
    "                                                 replace_numbers=False,\n",
    "                                                 replace_hashtags=False)\n",
    "        \n",
    "        self.lemmatizer = Lemmatizer()\n",
    "\n",
    "        self.text_refinement_patterns = text_refinement_patterns\n",
    "        if self.text_refinement_patterns:\n",
    "            self.text_refinement_patterns = compile_patterns(self.text_refinement_patterns)\n",
    "\n",
    "    def normalize(self, text: str):\n",
    "        text = text.translate(self.user_translations)\n",
    "        text = text.translate(self.number_translations)\n",
    "\n",
    "        # convert all presion numbers to english numbers or reverse\n",
    "        \n",
    "        text = text.lower()\n",
    "\n",
    "        normalized_text = self.text_normalizer.normalize(text)\n",
    "\n",
    "        if self._remove_repetitive_chars:\n",
    "            text = self.remove_rep_chars(text)\n",
    "\n",
    "        if self._change_lang_spacing:\n",
    "            text = self.change_lang_spacing(text)\n",
    "\n",
    "        if self._remove_non_standard_char:\n",
    "            text = self.remove_non_standard_char(text)\n",
    "\n",
    "        if self.text_refinement_patterns:\n",
    "            for pattern, repl in self.text_refinement_patterns:\n",
    "                text = pattern.sub(repl, text)\n",
    "\n",
    "        # reduce multiple spaces to one space\n",
    "        text = re.sub(r'[\\u200c\\s]*\\s[\\s\\u200c]*', ' ', text)\n",
    "        text = re.sub(r'[\\u200c]+', '\\u200c', text)\n",
    "\n",
    "        return text\n",
    "\n",
    "    def tokenize_text(self, text: str):\n",
    "        return self.word_tokenizer.tokenize(text)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def change_lang_spacing(text: str) -> str:\n",
    "        return re.sub('(([a-zA-Z0-9/\\-\\.]+)|([ء-یژپچگ]+))', r' \\1 ', text).strip()\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_non_standard_char(text: str) -> str:\n",
    "        # replace every junk character with space (all characters except Persian and English chars plus English digits)\n",
    "        return re.sub(r'[^a-zA-Z0-9\\u0621-\\u06CC\\u0698\\u067E\\u0686\\u06AF/\\-]\\.', \" \", text)\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_rep_chars(text: str) -> str:\n",
    "        return re.sub(r'([^0-9])\\1\\1+', r'\\1', text)\n",
    "    \n",
    "    def preprocess_text(self, text: str):\n",
    "        normalized_text = self.normalize(text)\n",
    "#         words = self.tokenize_text(normalized_text)\n",
    "#         lemmatized_words = [self.lemmatizer.lemmatize(word) for word in words]\n",
    "        return normalized_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now preprocess all data using `TextHandler.preprocess_text` function and add a new preprocessed_text column to your data. Read ```input-data/my_translation_dict.pickle``` and pass it to TextHandler with arguments ```change_lang_spacing=True, remove_non_standard_char=True, persian_numbers=False, user_translations=new_translation.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "text_handler = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use pandas ```apply``` functions and call ```text_handler.preprocess_text``` on ```text``` column to add new ```preprocessed_text``` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "data['preprocessed_text'] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[4, 'preprocessed_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to save your data for further use. You may want to use `to_parquet` built in function of pandas DataFrame. Save it on `\"./outputs/preprocessed_data.parquet\"` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is now cleaned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Part: Transforming the text into vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read preprocessed_data from ```./outputs/preprocessed_data.parquet``` and name it `data` and assing new values to ```id``` column so that it represent the row number. (starting from zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "data = \n",
    "data['id'] = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to create specific vector from every document.\n",
    "\n",
    "As we talked, first we are going to use **Count Vectorizing** method. `scikit learn` package has an implementation, which is `CountVectorizer` class.\n",
    "\n",
    "```python\n",
    "class sklearn.feature_extraction.text.CountVectorizer(input=’content’, encoding=’utf-8’, decode_error=’strict’, strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern=’(?u)\\b\\w\\w+\\b’, ngram_range=(1, 1), analyzer=’word’, max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class ‘numpy.int64’>)\n",
    "```\n",
    "Convert a collection of text documents to a matrix of token counts\n",
    "\n",
    "This implementation produces a sparse representation of the counts using scipy.sparse.csr_matrix.\n",
    "\n",
    "If you do not provide an a-priori dictionary and you do not use an analyzer that does some kind of feature selection then the number of features will be equal to the vocabulary size found by analyzing the data.\n",
    "\n",
    "\n",
    "\n",
    "**min_df** : float in range [0.0, 1.0] or int, default=1 <br>\n",
    "When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None. <br>\n",
    "For further information see [CountVectorizer-Documentation](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html).\n",
    "\n",
    "\n",
    "Run cell below to get `vectorized_description_tf`, which is expected vectors for every document using **Count Vectorizing** method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect_tf = CountVectorizer(min_df=50).fit(data.preprocessed_text)\n",
    "vectorized_description_tf = vect_tf.transform(data.preprocessed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity between Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you could see, `vectorized_description_tf` is a `sparse.csr_matrix`. Now we want to compute most similar ads for a specific ad. \n",
    "\n",
    "So we want to write a function to get all clean `data`, all `vectorized_description`, a specific `id`, and number `k` as inputs, and return `k` most similar ads to ad with  id`=id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "def get_top_similar_docs(data, vectorized_description, package_name, k):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see some examples. Try with different `ids` that you can find from original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_df = get_top_similar_docs(data=data, vectorized_description=vectorized_description_tf,\n",
    "                                   id=1, k=10)\n",
    "examples_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to improve our algorithm. We are going to use **TF-IDF** vectors from docs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please read some words from wikipedia, [TF-IDF-wikipedia](https://en.wikipedia.org/wiki/Tf–idf):\n",
    "\n",
    "\n",
    "\n",
    "#### Motivations\n",
    "\n",
    "##### Term frequency\n",
    "Suppose we have a set of English text documents and wish to rank which document is most relevant to the query, \"the brown cow\". A simple way to start out is by eliminating documents that do not contain all three words \"the\", \"brown\", and \"cow\", but this still leaves many documents. To further distinguish them, we might count the number of times each term occurs in each document; the number of times a term occurs in a document is called its term frequency. However, in the case where the length of documents varies greatly, adjustments are often made (see definition below). The first form of term weighting is due to Hans Peter Luhn (1957) which may be summarized as:\n",
    "\n",
    "The weight of a term that occurs in a document is simply proportional to the term frequency.\n",
    "\n",
    "##### Inverse document frequency\n",
    "Because the term \"the\" is so common, term frequency will tend to incorrectly emphasize documents which happen to use the word \"the\" more frequently, without giving enough weight to the more meaningful terms \"brown\" and \"cow\". The term \"the\" is not a good keyword to distinguish relevant and non-relevant documents and terms, unlike the less-common words \"brown\" and \"cow\". Hence an inverse document frequency factor is incorporated which diminishes the weight of terms that occur very frequently in the document set and increases the weight of terms that occur rarely.\n",
    "\n",
    "Karen Spärck Jones (1972) conceived a statistical interpretation of term specificity called Inverse Document Frequency (IDF), which became a cornerstone of term weighting:\n",
    "\n",
    "The specificity of a term can be quantified as an inverse function of the number of documents in which it occurs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now use TfidfVectorizer instead of CountVectorizer to vectorize preprocessed_text column. Pass ```min_df=50``` to ignore terms with documents frequencies less than 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# TODO: \n",
    "vect_tf_idf = \n",
    "vectorized_description_tf_idf = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use your `get_top_similar_docs` function, and this time try it with `vectorized_description=vectorized_description_tf_idf` to see new results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_df = get_top_similar_docs(data=data, vectorized_description=vectorized_description_tf_idf,\n",
    "                                   id=40, k=10)\n",
    "examples_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare two methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "score={}\n",
    "doc = list(data[data['id'] == 10]['preprocessed_text'])[0]\n",
    "X = vect_tf.transform([doc])\n",
    "for word in doc.split():\n",
    "    if word in set(vect_tf.vocabulary_.keys()):\n",
    "        score[word] = X[0, vect_tf.vocabulary_[word]]\n",
    "sortedscore = sorted(score.items(), key=operator.itemgetter(1), reverse=True)\n",
    "for item in sortedscore:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "score={}\n",
    "doc = list(data[data['id'] == 10]['preprocessed_text'])[0]\n",
    "X = vect_tf_idf.transform([doc])\n",
    "for word in doc.split():\n",
    "    if word in set(vect_tf_idf.vocabulary_.keys()):\n",
    "        score[word] = X[0, vect_tf_idf.vocabulary_[word]]\n",
    "sortedscore = sorted(score.items(), key=operator.itemgetter(1), reverse=True)\n",
    "for item in sortedscore:\n",
    "    print(item)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third Part: Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "\n",
    "Word2Vec algorithm is used for learning vector representations of words called “word embeddings”.\n",
    "\n",
    "You can read more about it in the following links:\n",
    "- [Word embedding](https://medium.com/data-science-group-iitr/word-embedding-2d05d270b285)\n",
    "- [Introduction to Word Embedding and Word2Vec](https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa)\n",
    "\n",
    "Also reading the original [paper](http://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization.pdf) is really recommended.\n",
    "\n",
    "You can also watch this [video](https://youtu.be/yexR53My2O4) and we recommend you start browsing and following some channels like [this](https://www.youtube.com/channel/UCZHmQk67mSJgfCCTn7xBfew). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using [this documentation](https://radimrehurek.com/gensim/models/word2vec.html), you can train word2vec model in gensim. Please use following options:\n",
    "- **skip-gram** -> True\n",
    "- **iterations** -> 10 \n",
    "- **vectors dimension** -> 100 \n",
    "\n",
    "Use the defualt for the other options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may want to use `gensim.models.Word2Vec`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "DIMENSION = 100\n",
    "w2v_model = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to explore in the model\n",
    "\n",
    "Use Word2Vec model methods such as `similar_by_word` to see some charachteristics of the model. For example find most similar words to ```پراید```, ```تلگرام```, ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save the model in ```outputs/word2vec.model```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to high dimensions of embedded words, visualizing is not that simple.\n",
    "\n",
    "There are some ways to map high dimensional vectors to say 2D plane. **PCA** is one of them, but we are not going to use it. Instead we use something that is called **t-SNE**.\n",
    "\n",
    "You can read more about it in the following links:\n",
    "- [Visualising high-dimensional datasets using PCA and t-SNE in Python](https://medium.com/@luckylwk/visualising-high-dimensional-datasets-using-pca-and-t-sne-in-python-8ef87e7915b)\n",
    "- [How to Use t-SNE Effectively](https://distill.pub/2016/misread-tsne/)\n",
    "\n",
    "Use `TSNE` object class from `MulticoreTSNE` package to visualize your word-vectors. Use proper values for `TSNE` parameters like `n_jobs`, `verbose`, `perplexity`, and `n_iter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "w2v_model = Word2Vec.load(\"outputs/divar_word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MulticoreTSNE import MulticoreTSNE as TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "tsne = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For applying the job, you may need to use `fit_transform` module from `TSNE` class. Use fit_trnsform and put the results in projections variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "projections ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to plot all the data.\n",
    "\n",
    "Run cell below to see the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter(\n",
    "    projections, x=0, y=1, hover_name=w2v_model.wv.index_to_key\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not good enough?\n",
    "\n",
    "You can define some keywords, and use `most_similar` method to get `topn=100` most similar words for each keyword, and only plot those words.\n",
    "\n",
    "First define your preferred keyword list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "keywords_lst = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every keyword you may want to get the index of top 100 most similiar words and add it to selected indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_indexes = set()\n",
    "\n",
    "for word in keywords_lst:\n",
    "    # TODO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_indexes = list(selected_indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can plot only these words representations with a few modification on inputs of px.scatter() in above cells. Try it yourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making vector for the docs (by word2vec)\n",
    "\n",
    "After turning each word to a vector, it is time to compute vector for each document. Turn each document to a vector by just adding its words' vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to find similar ads\n",
    "\n",
    "It is time for another `get_top_similar_docs` function. Implement it for using the new doc vectors and name it `get_w2v_top_similar_docs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "def get_w2v_top_similar_docs(data, vectorized_description, package_name, k):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See some results!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "examples_df = \n",
    "examples_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another way for making docs' vectors\n",
    "\n",
    "Instead of just summing up each word's vector, we can sum them with a weight, for example their IDFs. Also, for having much more speed, we can do the math using matrixes!\n",
    "Let's do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, see some results with these new vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastText\n",
    "\n",
    "\n",
    "Another way to learn word embeddings is FastText. \n",
    "\n",
    "FastText is a library for efficient learning of word representations and sentence classification.\n",
    "\n",
    "In order to install FastText you may want to run commands below:\n",
    "```\n",
    "$ git clone https://github.com/facebookresearch/fastText.git\n",
    "$ cd fastText\n",
    "$ mkdir build && cd build && cmake ..\n",
    "$ make && make install\n",
    "```\n",
    "\n",
    "These links will be helpful for becoming familiar with this library:\n",
    " - [Documentation](https://fasttext.cc/docs/en/support.html)\n",
    " - [Git Repo](https://github.com/facebookresearch/fastText)\n",
    "\n",
    "One could find some blogs on medium, and etc. too:\n",
    " - [Learning FastText](https://towardsdatascience.com/fasttext-ea9009dba0e8)\n",
    " - [FastText: Under the Hood](https://towardsdatascience.com/fasttext-under-the-hood-11efc57b2b3)\n",
    "\n",
    "Also reading the original [papers](https://research.fb.com/downloads/fasttext/) is really recommended.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to train FastText vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use bash commands on our notebook cells with `%sh`.\n",
    "\n",
    "First write `preprocessed_text` column in `data` in `outputs/preprocessed_text_fasttext.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now can train FastText model with `outputs/preprocessed_text_fasttext.txt` as input file, and your desired hyperparameters, and **cbow**, or **skipgram** approaches. \n",
    "\n",
    "Save your model to `outputs/fasttext-model`.\n",
    "\n",
    "You may want to set hyperparameters as follows:\n",
    "- **minCount** -> 10 \n",
    "- **minn** -> 4 \n",
    "- **maxn** -> 6\n",
    "- **neg** -> 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "%%sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load your trained model with `load_fasttext_format` from `gensim FastText model` and pass in the `bin` file created from the last cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO \n",
    "from gensim.models.wrappers import FastText\n",
    "\n",
    "fasttext_model = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again use FastText model methods such as `most_similar` and `similar_by_word` to see some charachteristics of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to find word vectors with FastText pretrained model.\n",
    "\n",
    "You may download FastText pretrained model for many different language from [Word vectors for 157 languages](https://fasttext.cc/docs/en/crawl-vectors.html). We are obviously going to use persian pretrained model which you can download the [bin](https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fa.300.bin.gz) file.\n",
    "\n",
    "Quite a huge file tough :D.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use `print-word-vectors` from `fasttext` library, and we need to pass in the pretrained vectors with our vocabulary.\n",
    "\n",
    "Create `outputs/processed_words.txt` and set it to be the all distinct word from `data['preprocessed_text']`. Each word in one line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use `print-word-vectors` from `fasttext` and save the results to `./outputs/fasttext_pretrained_vectors.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "%%sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to add distinct words count, and vector dimension (which is 300) to the head of `./outputs/fasttext_pretrained_vectors.txt` so gensim could read the vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "%%sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "%%sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load `./outputs/fasttext_pretrained_vectors.txt` with `load_word2vec_format` from `gensim KeyedVectors model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "fasttext_pretrained_model = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkout some charachteristics of this model too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also plot the t-SNE 2D representation of `fasttext_pretrained_model` and compare it with `Word2Vec`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - The End."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "divar-nlp-kernel",
   "language": "python",
   "name": "divar-nlp-workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
